use_amp: True
use_ema: False

ema:
  type: ModelEMA
  decay: 0.9999
  warmups: 1000
  start: 0

# Reduced for faster training
epoches: 25
clip_max_norm: 0.1

optimizer:
  type: AdamW
  params:
    - {params: '^(?=.*backbone)(?!.*norm).*$', lr: 0.0000125}
    - {params: '^(?=.*(?:encoder|decoder))(?=.*(?:norm|bn)).*$', weight_decay: 0.}

  lr: 0.00025
  betas: [0.9, 0.999]
  weight_decay: 0.000125

lr_scheduler:
  type: MultiStepLR
  milestones: [20]  # Step decay before final epochs
  gamma: 0.1

lr_warmup_scheduler:
  type: LinearWarmup
  warmup_duration: 200  # Reduced warmup for shorter training
